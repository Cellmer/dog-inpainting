{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Inpainting\n",
    "Uczenie Głębokie - praca domowa<br />\n",
    "Autorzy:<br />\n",
    "*Jakub Link 184469*<br />\n",
    "*Michał Cellmer 184685*<br />\n",
    "specjalność: Uczenie Maszynowe<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:35:52.073264500Z",
     "start_time": "2025-01-17T12:35:47.491508200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\miniconda3\\envs\\dog-inpainting\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pozyskanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'archive'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path = kagglehub.dataset_download(\"wutheringwang/dog-face-recognition\")\n",
    "shutil.move(cache_path, \"archive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:36:04.254690900Z",
     "start_time": "2025-01-17T12:36:04.181400300Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 64\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "mask_fraction_range = (0.15, 0.3)\n",
    "\n",
    "data_dir = \"./archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie wstępne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:36:06.868570100Z",
     "start_time": "2025-01-17T12:36:06.853295700Z"
    }
   },
   "outputs": [],
   "source": [
    "class DogDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.image_files = []\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            self.image_files.extend([os.path.join(root, f) for f in files if f.endswith('.jpg')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Mask generation\n",
    "        mask = np.zeros((image_size, image_size), dtype=np.float32)\n",
    "        mask_fraction = np.random.uniform(*mask_fraction_range)\n",
    "        mask_size = int(image_size * mask_fraction)\n",
    "        top = np.random.randint(0, image_size - mask_size)\n",
    "        left = np.random.randint(0, image_size - mask_size)\n",
    "        mask[top:top + mask_size, left:left + mask_size] = 1\n",
    "\n",
    "        masked_image = image * (1 - mask)\n",
    "        return (\n",
    "            torch.tensor(masked_image).unsqueeze(0),\n",
    "            torch.tensor(image).unsqueeze(0),\n",
    "            torch.tensor(mask).unsqueeze(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:36:08.240901700Z",
     "start_time": "2025-01-17T12:36:08.024372100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DogDataset(os.path.join(data_dir, \"train\"))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = DogDataset(os.path.join(data_dir, \"test_200_single_img\"))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja modelu autokodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:36:09.752338Z",
     "start_time": "2025-01-17T12:36:09.726746600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:36:13.170212300Z",
     "start_time": "2025-01-17T12:36:12.009618300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for masked_images, original_images, masks in tqdm(loader, desc=\"Training\"):\n",
    "        masked_images = masked_images.to(device)\n",
    "        original_images = original_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(masked_images)\n",
    "        loss = loss_fn(outputs * masks, original_images * masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def val_step(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.inference_mode():\n",
    "        for masked_images, original_images, masks in tqdm(loader, desc=\"Validation\"):\n",
    "            masked_images = masked_images.to(device)\n",
    "            original_images = original_images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(masked_images)\n",
    "            loss = loss_fn(outputs * masks, original_images * masks)\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:41:43.679388900Z",
     "start_time": "2025-01-17T12:36:18.838633900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [03:58<00:00,  2.39it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:02<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0011, Validation Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:37<00:00, 15.23it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.0007, Validation Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:23<00:00, 24.66it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 33.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Training Loss: 0.0007, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:24<00:00, 23.06it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:27<00:00, 20.46it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 30.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:28<00:00, 20.13it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:24<00:00, 23.13it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:24<00:00, 23.08it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:24<00:00, 23.17it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 28.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Training Loss: 0.0006, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:31<00:00, 17.85it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:28<00:00, 20.32it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 28.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Training Loss: 0.0006, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:30<00:00, 18.99it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Training Loss: 0.0006, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:26<00:00, 21.53it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 31.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Training Loss: 0.0006, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:25<00:00, 22.72it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Training Loss: 0.0006, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:27<00:00, 21.10it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Training Loss: 0.0006, Validation Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:27<00:00, 20.62it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 32.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Training Loss: 0.0005, Validation Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:31<00:00, 18.16it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Training Loss: 0.0005, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:28<00:00, 19.86it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Training Loss: 0.0005, Validation Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:26<00:00, 21.76it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Training Loss: 0.0005, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 571/571 [00:25<00:00, 22.16it/s]\n",
      "Validation: 100%|██████████| 6/6 [00:00<00:00, 20.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Training Loss: 0.0005, Validation Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    avg_train_loss = train_step(model, train_loader, optimizer, loss_fn, device)\n",
    "    avg_val_loss = val_step(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Wizualizacja przykładów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:15:46.171987300Z",
     "start_time": "2025-01-17T13:15:46.110495800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_files = []\n",
    "        self.mask_files = []\n",
    "        \n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            self.image_files.extend([os.path.join(root, f) for f in files if f.endswith('.jpg')])\n",
    "        for root, _, files in os.walk(mask_dir):\n",
    "            self.mask_files.extend([os.path.join(root, f) for f in files if f.endswith('.jpg')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        mask_path = self.mask_files[idx]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (image_size, image_size))\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "\n",
    "        masked_image = image * (1 - mask)\n",
    "        return (\n",
    "            torch.tensor(masked_image).unsqueeze(0),\n",
    "            torch.tensor(image).unsqueeze(0),\n",
    "            torch.tensor(mask).unsqueeze(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:15:48.085910400Z",
     "start_time": "2025-01-17T13:15:48.025868200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_dataset = EvalDataset(os.path.join(\"tests_dogs\", \"images_dogs\"), os.path.join(\"tests_dogs\", \"masks_dogs\"))\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:15:49.209712400Z",
     "start_time": "2025-01-17T13:15:49.150024400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inpaint_image2(model, image, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0).to(device))\n",
    "        inpainted_image = output.squeeze(0).cpu()\n",
    "        result = image * (1 - mask) + inpainted_image * mask\n",
    "        return inpainted_image, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:15:51.740408800Z",
     "start_time": "2025-01-17T13:15:51.102143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "present_size = 128\n",
    "presentation = np.zeros((present_size*5, present_size*8, 1), dtype=np.float32)\n",
    "\n",
    "for idx, (masked_raw, raw, mask) in enumerate(eval_loader):\n",
    "    generated, combined = inpaint_image2(model, masked_raw.squeeze(), mask)\n",
    "    \n",
    "    left = (idx % 2) * present_size * 4\n",
    "    top = (idx // 2) * present_size\n",
    "    \n",
    "    presentation[top:top+present_size, left:left+present_size, :] = cv2.resize(raw.squeeze().cpu().numpy(), (present_size, present_size)).reshape((present_size, present_size, 1))\n",
    "    presentation[top:top+present_size, left+present_size:left+2*present_size, :] = cv2.resize(mask.squeeze().cpu().numpy(), (present_size, present_size)).reshape((present_size, present_size, 1))\n",
    "    presentation[top:top+present_size, left+2*present_size:left+3*present_size, :] = cv2.resize(generated.squeeze().cpu().numpy(), (present_size, present_size)).reshape((present_size, present_size, 1))\n",
    "    presentation[top:top+present_size, left+3*present_size:left+4*present_size, :] = cv2.resize(combined.squeeze().cpu().numpy(), (present_size, present_size)).reshape((present_size, present_size, 1))\n",
    "\n",
    "cv2.imwrite(\"tests_dogs.jpg\", presentation.reshape((present_size*5, present_size*8))*255.)\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(cv2.imread(\"tests_dogs.jpg\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-inpainting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
